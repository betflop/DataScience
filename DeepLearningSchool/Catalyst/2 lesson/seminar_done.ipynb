{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"seminar_done.ipynb","provenance":[],"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"zwRfuC6RDb1B"},"source":["# Seminar 2. PyTorch\n","Hi! Today we are going to study PyTorch. We'll compare numpy and PyTorch commands, rewrite our previous neural network in two ways.\n","\n","!!! GPU ON !!!"]},{"cell_type":"code","metadata":{"id":"ilG0uJwVcke4"},"source":["!pip install mnist"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cMKPD9TZdokn"},"source":["from IPython import display\n","import numpy as np\n","import random\n","import torch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7gsxpRimFHF-"},"source":["np.random.seed(42)\n","random.seed(42)\n","torch.manual_seed(42)\n","torch.cuda.manual_seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b7xxXYAeFofM"},"source":["## Numpy vs Pytorch\n","### Initialization"]},{"cell_type":"code","metadata":{"id":"kbIPHFs6FmkB"},"source":["a = [1. , 1.4 , 2.5]\n","print(f\"Simple way: {torch.tensor(a)}\")\n","print(f\"Zeros:\\n {torch.zeros((2,3))}\")\n","print(f\"Range: {torch.arange(0, 10)}\")\n","print(f\"Complicated range: {torch.arange(4, 12, 2)}\")\n","print(f\"Space: {torch.linspace(1, 4, 6)}\")\n","print(f\"Identity matrix:\\n {torch.eye(4)}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sWKCkXaSGK1U"},"source":["### Random"]},{"cell_type":"code","metadata":{"id":"OHFgnpzOF5yb"},"source":["print(f\"From 0 to 1: {torch.rand(1)}\")\n","print(f\"Vector from 0 to 1: {torch.rand(5)}\")\n","print(f\"Vector from 0 to 10: {torch.randint(10, size=(5,))}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7EmP3g0wGfzT"},"source":["### Matrix Operation"]},{"cell_type":"code","metadata":{"id":"KfUdENnEF8-l"},"source":["a = torch.arange(10).type(torch.FloatTensor)\n","b = torch.linspace(-10, 10, 10)\n","print(f\"a: {a}\\nshape: {a.size()}\")\n","print(f\"b: {a}\\nshape: {b.size()}\")\n","print(f\"a + b: {a + b},\\n a * b: {a * b}\")\n","print(f\"Dot product: {a.dot(b)}\")\n","print(f\"Mean: {a.mean()}, STD: {a.std()}\")\n","print(f\"Sum: {a.sum()}, Min: {a.min()}, Max: {a.max()}\")\n","print(f\"Reshape:\\n{a.reshape(-1, 1)}\\nshape: {a.reshape(-1, 1).size()}\")\n","c = a.reshape(-1, 1).repeat(1, 5)\n","print(f\"Repeat:\\n{c}\\nshape: {c.size()}\")\n","print(f\"Transpose:\\n{c.T}\\nshape: {c.T.size()}\")\n","print(f\"Unique items: {torch.unique(c)}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pdhrhXj9H0m6"},"source":["### Indexing"]},{"cell_type":"code","metadata":{"id":"bHqXM0bQGrRv"},"source":["a = torch.arange(100).reshape(10, 10)\n","print(f\"Array:\\n{a}\\nshape: {a.size()}\")\n","print(f\"Get first column: {a[:, 0]}\")\n","print(f\"Get last row: {a[-1, :]}\")\n","print(f\"Add new awis:\\n{a[:, np.newaxis]}\\nshape: {a[:, np.newaxis].size()}\")\n","print(f\"Specific indexing:\\n{a[4:6, 7:]}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rFQ0OwEkIHu8"},"source":["### Numpy <-> Pytorch"]},{"cell_type":"code","metadata":{"id":"oueWvyMzH6sZ"},"source":["a = torch.normal(mean=torch.zeros(2,4))\n","a.numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VRCr6c78QeH1"},"source":["b = np.random.normal(size=(2, 4))\n","torch.from_numpy(b)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fJyktewrROPL"},"source":["### CUDA"]},{"cell_type":"code","metadata":{"id":"JshOGbX0RAb1"},"source":["a = torch.normal(mean=torch.zeros(2,4))\n","b = torch.normal(mean=torch.zeros(2, 4))\n","print(f\"a:\\n{a}\\nb:\\n{b}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mKUxAfveR6iQ"},"source":["a = a.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"klvLnHmaSDbk"},"source":["a + b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"11l9FxZVSEDL"},"source":["(a + b.cuda()).cpu()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DFkGBACsSmL4"},"source":["### Autograd"]},{"cell_type":"code","metadata":{"id":"Yr46uuNNSR_U"},"source":["a = torch.randn(2, requires_grad=True)\n","b = torch.normal(mean=torch.zeros(2))\n","\n","c = torch.dot(a, b)\n","print(f'a:\\n{a}\\nb:\\n{b}\\n(a,b): {c}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jmiq2lLnSvO4"},"source":["c.backward()\n","print(f'a:\\n{a}\\nb:\\n{b}\\n(a,b): {c}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O8I2iLEyS3dU"},"source":["print(f\"Grad a: {a.grad}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YFD2nov7UDlf"},"source":["Add function!"]},{"cell_type":"code","metadata":{"id":"-z2P8UECTnEO"},"source":["a = torch.randn(2, requires_grad=True)\n","b = torch.normal(mean=torch.zeros(2))\n","c = torch.ones(1, requires_grad=True)\n","\n","d = torch.sigmoid(torch.dot(a, b) + c)\n","print(f'a:\\n{a}\\nb:\\n{b}\\nSigmoid( (a,b) ): {d}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ur_QA6xbTzeG"},"source":["print(f\"Grad a: {a.grad}\\nGrad c: {c.grad}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PsheM7soT6sw"},"source":["d.backward()\n","print(f\"Grad a: {a.grad}\\nGrad c: {c.grad}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yf9pO1XJUFZI"},"source":["Okay, what about vectors?"]},{"cell_type":"code","metadata":{"id":"9ginrHkBT_HC"},"source":["a = torch.randn(2, requires_grad=True)\n","b = torch.normal(mean=torch.zeros(2))\n","\n","c = a * b\n","c.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-sNYtrJ3YxV3"},"source":["## Neural Network. Rewind"]},{"cell_type":"code","metadata":{"id":"_MKsdG6GdorV"},"source":["from copy import deepcopy\n","\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","import seaborn as sns\n","\n","import mnist\n","\n","sns.set()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"waQtAlplIh6N"},"source":["images = mnist.train_images() / 255\n","labels = mnist.train_labels()\n","\n","X_train, X_valid, y_train, y_valid = train_test_split(images, labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BOHbffeqHyeF"},"source":["def get_batches(dataset, batch_size):\n","    X = dataset[0].reshape(-1, 28 * 28)\n","    Y = dataset[1]\n","    n_samples = X.shape[0]\n","    \n","    indices = np.arange(n_samples)\n","    np.random.shuffle(indices)\n","    \n","    for start in range(0, n_samples, batch_size):\n","        end = min(start + batch_size, n_samples)\n","        \n","        batch_idx = indices[start:end]\n","    \n","        yield torch.FloatTensor(X[batch_idx]), torch.LongTensor(Y[batch_idx])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R402HnG8ULdl"},"source":["class CustomLinear:\n","    def __init__(self, in_size, out_size):\n","        \"\"\"\n","        Simple linear layer\n","        \"\"\"\n","        self.in_size = in_size\n","        self.out_size = out_size\n","\n","        self.w = torch.randn((in_size, out_size), requires_grad=True)\n","        self.b = torch.randn((1, out_size), requires_grad=True)\n","\n","    def __call__(self, x):\n","        return torch.mm(x, self.w) + self.b\n","\n","    def zero_grad(self):\n","        self.w.grad = None\n","        self.b.grad = None"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vzDzCXmtZivn"},"source":["class CustomNeuralNetwork:\n","    def __init__(self, dims, activation=\"sigmoid\"):\n","        \"\"\"\n","        Simple deep networks, that joins several linear layers. \n","        \"\"\"\n","        self.dims = dims\n","\n","        self.linears = list(CustomLinear(d_0, d_1) for d_0, d_1 in zip(dims[:-1], dims[1:]))\n","        if activation == \"sigmoid\":\n","            self.activation = torch.sigmoid\n","        elif activation == \"relu\":\n","            self.activation = torch.relu\n","        else:\n","            raise NotImplementedError\n","\n","    def __call__(self, x):\n","        for l in self.linears[:-1]:\n","            x = self.activation(l(x))\n","        return self.linears[-1](x)\n","\n","    def zero_grad(self):\n","        for l in self.linears:\n","            l.zero_grad()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LZtMZC_dbmFS"},"source":["class CustomCrossEntropy:\n","    def __call__(self, x, target):\n","        x = torch.log_softmax(x, 1)\n","        return - torch.mean(x[torch.arange(target.size(0)), target])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t47gvIlUkFpT"},"source":["def simple_sgd(model, config):\n","    with torch.no_grad():\n","        for l in model.linears:\n","            l.w -= config['learning_rate'] * l.w.grad\n","            l.b -= config['learning_rate'] * l.b.grad"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ER9Cs5jaoNi"},"source":["net = CustomNeuralNetwork((28 * 28, 10, 10))\n","criterion = CustomCrossEntropy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gOQeiTU0Jnkr"},"source":["optimizer_config = {\n","    \"learning_rate\": 1e-1\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5nwAo4jfaxry"},"source":["def train(model, optimizer_config, n_epoch=20, batch_size=256):\n","    train_logs = {\"Train Loss\": [0,], \"Steps\": [0,]}\n","    valid_logs = {\"Valid Loss\": [0,], \"Valid Accuracy\": [0,], \"Steps\": [0,]}\n","    step = 0\n","    best_valid_loss = np.inf\n","    best_model = None\n","\n","    for i in range(n_epoch):\n","        for x_batch, y_batch in get_batches((X_train, y_train), batch_size):\n","            model.zero_grad()\n","            \n","            predictions = model(x_batch)\n","            loss = criterion(predictions, y_batch)\n","\n","            loss.backward()\n","            \n","            simple_sgd(model, optimizer_config)      \n","            \n","            step += 1\n","            train_logs[\"Train Loss\"].append(loss.detach().item())\n","            train_logs[\"Steps\"].append(step)\n","\n","        sum_loss = 0\n","        sum_acc = 0\n","        count_valid_steps = 0\n","        with torch.no_grad():\n","            for x_batch, y_batch in get_batches((X_valid, y_valid), batch_size):\n","                predictions = model(x_batch)\n","                loss = criterion(predictions, y_batch)\n","                sum_loss += loss.item()\n","                sum_acc += accuracy_score(y_batch, np.argmax(predictions.numpy(), axis=1))\n","                count_valid_steps += 1\n","\n","        valid_logs[\"Valid Loss\"].append(sum_loss / count_valid_steps)\n","        valid_logs[\"Valid Accuracy\"].append(sum_acc / count_valid_steps)\n","        valid_logs[\"Steps\"].append(step)\n","\n","        if best_valid_loss > sum_loss / count_valid_steps:\n","            best_valid_loss = sum_loss / count_valid_steps\n","            best_model = deepcopy(model)\n","\n","    fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n","    sns.lineplot(x=\"Steps\", y=\"Train Loss\", data=train_logs, ax=ax[0])\n","    sns.lineplot(x=\"Steps\", y=\"Valid Loss\", data=valid_logs, ax=ax[1])\n","    sns.lineplot(x=\"Steps\", y=\"Valid Accuracy\", data=valid_logs, ax=ax[2])\n","    plt.plot()\n","\n","    return best_model, train_logs, valid_logs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ufy_8neBa3hG"},"source":["net, _, _ = train(net, optimizer_config)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oWnI3eIEeLZT"},"source":["## Neural Network. Rewind #2"]},{"cell_type":"code","metadata":{"id":"sLgLKCUfcCG3"},"source":["import torch.nn as nn\n","\n","\n","class Linear(nn.Module):\n","    def __init__(self, in_size, out_size):\n","        super().__init__()\n","\n","        self.in_size = in_size\n","        self.out_size = out_size\n","\n","        self.w = nn.Parameter(torch.randn(in_size, out_size))\n","        self.b = nn.Parameter(torch.randn(out_size))\n","\n","    def forward(self, x):\n","        return torch.mm(x, self.w) + self.b\n","\n","\n","class BatchNorm(nn.Module):\n","    def __init__(self, in_size, alpha=0.1):\n","        super().__init__()\n","\n","        self.in_size = in_size\n","\n","        self.beta = nn.Parameter(torch.zeros(in_size))\n","        self.gamma = nn.Parameter(torch.ones(1, in_size))\n","\n","        self.epsilon = 1e-5\n","\n","    def forward(self, x):\n","        x = (x - x.mean(dim=0)) / torch.sqrt(x.var(dim=0) + self.epsilon)\n","        return x * self.gamma + self.beta\n","\n","\n","class Dropout(nn.Module):\n","    def __init__(self, p=0.5):\n","        super().__init__()\n","\n","        self.p = p\n","\n","    def forward(self, x):\n","        if self.training:\n","            binomial = torch.distributions.binomial.Binomial(probs=1-self.p)\n","            return x * binomial.sample(x.size()).to(x.device) * (1.0/(1-self.p))\n","        return x\n","\n","\n","class Block(nn.Module):\n","    def __init__(self, in_size, out_size, activation=\"sigmoid\", p=0.5):\n","        super().__init__()\n","\n","        self.linear = Linear(in_size, out_size)\n","        self.dropout = Dropout(p)\n","        self.batch_norm = BatchNorm(out_size)\n","        \n","        if activation == \"sigmoid\":\n","            self.activation = torch.sigmoid\n","        elif activation == \"relu\":\n","            self.activation = torch.relu\n","        else:\n","            raise NotImplementedError\n","\n","    def forward(self, x):\n","        x = self.linear(x)\n","        x = self.batch_norm(x)\n","        x = self.dropout(x)\n","        return self.activation(x)\n","\n","\n","class Net(nn.Module):\n","    def __init__(self, dims, activation=\"relu\", p=0.5):\n","        super().__init__()\n","\n","        self.blocks = nn.ModuleList(\n","            list(Block(d_0, d_1, activation=activation, p=p) for d_0, d_1 in zip(dims[:-2], dims[1:-1]))\n","        )\n","        self.cl = Linear(dims[-2], dims[-1])\n","\n","    def forward(self, x):\n","        for m in self.blocks:\n","            x = m(x)\n","        return self.cl(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PcqQ6AwPczjI"},"source":["net = Net((28 * 28, 100, 10), p=0.1).cuda()\n","criterion = nn.CrossEntropyLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cZtluygsczgm"},"source":["optimizer = torch.optim.SGD(net.parameters(), lr=1e-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o2iqWQojL3J7"},"source":["def train(model, optimizer, n_epoch=20, batch_size=256, device=\"cpu\"):\n","    train_logs = {\"Train Loss\": [0,], \"Steps\": [0,]}\n","    valid_logs = {\"Valid Loss\": [0,], \"Valid Accuracy\": [0,], \"Steps\": [0,]}\n","    step = 0\n","    best_valid_loss = np.inf\n","    best_model = None\n","\n","    for i in range(n_epoch):\n","        for x_batch, y_batch in get_batches((X_train, y_train), batch_size):\n","            x_batch = x_batch.to(device)\n","            y_batch = y_batch.to(device)\n","\n","            optimizer.zero_grad()\n","            \n","            predictions = model(x_batch)\n","            loss = criterion(predictions, y_batch)\n","\n","            loss.backward()\n","            \n","            optimizer.step()   \n","            \n","            step += 1\n","            train_logs[\"Train Loss\"].append(loss.detach().item())\n","            train_logs[\"Steps\"].append(step)\n","\n","        sum_loss = 0\n","        sum_acc = 0\n","        count_valid_steps = 0\n","        with torch.no_grad():\n","            for x_batch, y_batch in get_batches((X_valid, y_valid), batch_size):\n","                x_batch = x_batch.to(device)\n","                y_batch = y_batch.to(device)\n","\n","                predictions = model(x_batch)\n","                loss = criterion(predictions, y_batch)\n","                sum_loss += loss.item()\n","                sum_acc += accuracy_score(y_batch.cpu().numpy(), np.argmax(predictions.cpu().numpy(), axis=1))\n","                count_valid_steps += 1\n","\n","            valid_logs[\"Valid Loss\"].append(sum_loss / count_valid_steps)\n","            valid_logs[\"Valid Accuracy\"].append(sum_acc / count_valid_steps)\n","            valid_logs[\"Steps\"].append(step)\n","\n","            if best_valid_loss > sum_loss / count_valid_steps:\n","                best_valid_loss = sum_loss / count_valid_steps\n","                best_model = deepcopy(net)\n","\n","    fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n","    sns.lineplot(x=\"Steps\", y=\"Train Loss\", data=train_logs, ax=ax[0])\n","    sns.lineplot(x=\"Steps\", y=\"Valid Loss\", data=valid_logs, ax=ax[1])\n","    sns.lineplot(x=\"Steps\", y=\"Valid Accuracy\", data=valid_logs, ax=ax[2])\n","    plt.plot()\n","\n","    return best_model, train_logs, valid_logs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QDkpB7TtL3IU"},"source":["net, _, _ = train(net, optimizer, device=\"cuda:0\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mcOEa7lcjiyO"},"source":["### Neural Network. Rewind #3. Logging\n","\n","Logging systems:\n","- [Tensorboard](https://pytorch.org/docs/stable/tensorboard.html)\n","- [WandB](https://www.wandb.com/)"]},{"cell_type":"code","metadata":{"id":"fjRHFJXSjn81"},"source":["class Block(nn.Module):\n","    def __init__(self, in_size, out_size, activation=\"relu\", p=0.5):\n","        super().__init__()\n","\n","        self.in_size = in_size\n","        self.out_size = out_size\n","\n","        if activation == \"sigmoid\":\n","            self.activation = nn.Sigmoid\n","        elif activation == \"relu\":\n","            self.activation = nn.ReLU\n","        else:\n","            raise NotImplementedError\n","\n","        self.fc = nn.Sequential(\n","            nn.Linear(self.in_size, self.out_size),\n","            nn.BatchNorm1d(self.out_size),\n","            nn.Dropout(p),\n","            self.activation()\n","        )\n","\n","    def forward(self, x):\n","        return self.fc(x)\n","\n","\n","net = nn.Sequential(Block(28 * 28, 100, p=0.2), nn.Linear(100, 10)).cuda()\n","optimizer = torch.optim.SGD(net.parameters(), lr=1e-1)\n","criterion = nn.CrossEntropyLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QV1AxNFbfapY"},"source":["%load_ext tensorboard"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cgo_htQZfamn"},"source":["from datetime import datetime\n","from pathlib import Path\n","\n","from torch.utils.tensorboard import SummaryWriter"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4qWa59wKf5Jd"},"source":["def train(model, optimizer, n_epoch=20, batch_size=256, device=\"cpu\"):\n","    writer = SummaryWriter(Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","    step = 0\n","    best_valid_loss = np.inf\n","    best_model = None\n","\n","    for i in range(n_epoch):\n","        for x_batch, y_batch in get_batches((X_train, y_train), batch_size):\n","            x_batch = x_batch.to(device)\n","            y_batch = y_batch.to(device)\n","\n","            optimizer.zero_grad()\n","            \n","            predictions = model(x_batch)\n","            loss = criterion(predictions, y_batch)\n","\n","            loss.backward()\n","            \n","            optimizer.step()   \n","            \n","            step += 1\n","            writer.add_scalar(\"Train Loss\", loss.detach().item(), step)\n","\n","        sum_loss = 0\n","        sum_acc = 0\n","        count_valid_steps = 0\n","        with torch.no_grad():\n","            for x_batch, y_batch in get_batches((X_valid, y_valid), batch_size):\n","                x_batch = x_batch.to(device)\n","                y_batch = y_batch.to(device)\n","\n","                predictions = model(x_batch)\n","                loss = criterion(predictions, y_batch)\n","                sum_loss += loss.item()\n","                sum_acc += accuracy_score(y_batch.cpu().numpy(), np.argmax(predictions.cpu().numpy(), axis=1))\n","                count_valid_steps += 1\n","\n","            writer.add_scalar(\"Valid Loss\", sum_loss / count_valid_steps, step)\n","            writer.add_scalar(\"Valid Accuracy\", sum_acc / count_valid_steps, step)\n","\n","            if best_valid_loss > sum_loss / count_valid_steps:\n","                best_valid_loss = sum_loss / count_valid_steps\n","                best_model = deepcopy(net)\n","\n","    return best_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qptL6E_Mf5G5"},"source":["net = train(net, optimizer, device=\"cuda:0\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SuzkIqc0f5FG"},"source":["%tensorboard --logdir logs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7W1zyKnXFdxt"},"source":["## Spoiler - train loop with [Catalyst](https://github.com/catalyst-team/catalyst)\n","\n","- [A comprehensive step-by-step guide to basic and advanced features](https://github.com/catalyst-team/catalyst#step-by-step-guide)\n","- [Docs](https://catalyst-team.github.io/catalyst/)\n","- [What is Runner?](https://catalyst-team.github.io/catalyst/api/core.html#runner)"]},{"cell_type":"code","metadata":{"id":"8Y_cQnMeJJiU"},"source":["!pip install catalyst"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gm2SIio7f5Cd"},"source":["import os\n","import torch\n","from torch.nn import functional as F\n","from torch.utils.data import DataLoader\n","from catalyst import dl\n","from catalyst.data.cv import ToTensor\n","from catalyst.contrib.datasets import MNIST\n","from catalyst.utils import metrics\n","\n","net = nn.Sequential(Block(28 * 28, 100, p=0.2), nn.Linear(100, 10))\n","optimizer = torch.optim.Adam(net.parameters(), lr=0.02)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","loaders = {\n","    \"train\": DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=ToTensor()), batch_size=32),\n","    \"valid\": DataLoader(MNIST(os.getcwd(), train=False, download=True, transform=ToTensor()), batch_size=32),\n","}\n","\n","class CustomRunner(dl.Runner):\n","\n","    def predict_batch(self, batch):\n","        # model inference step\n","        return self.model(batch[0].to(self.device).view(batch[0].size(0), -1))\n","\n","    def _handle_batch(self, batch):\n","        # model train/valid step\n","        x, y = batch\n","        y_hat = self.model(x.view(x.size(0), -1))\n","\n","        loss = self.criterion(y_hat, y)\n","        accuracy01, accuracy03 = metrics.accuracy(y_hat, y, topk=(1, 3))\n","        self.batch_metrics.update(\n","            {\"loss\": loss, \"accuracy01\": accuracy01, \"accuracy03\": accuracy03}\n","        )\n","\n","        if self.is_train_loader:\n","            loss.backward()\n","            self.optimizer.step()\n","            self.optimizer.zero_grad()\n","\n","runner = CustomRunner()\n","runner.train(\n","    model=net,\n","    criterion=criterion,\n","    optimizer=optimizer,\n","    loaders=loaders,\n","    logdir=Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n","    num_epochs=5,\n","    verbose=True,\n","    load_best_on_end=True,\n","    callbacks={\n","    \"optimizer\": dl.OptimizerCallback(\n","      metric_key=\"loss\",\n","      accumulation_steps=1,\n","      grad_clip_params=None,\n","    )\n","  }\n",")\n","\n","traced_model = runner.trace(loader=loaders[\"valid\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZUz3o_kJJIHe"},"source":[""],"execution_count":null,"outputs":[]}]}