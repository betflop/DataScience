{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"homework_embeddings.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"eYtJxkhKpYK2"},"source":["# Embeddings\n","\n","Привет! Сегодня ты поработаешь с эмбеддингами: сделаешь классификатор эмоции твитов. Для начала, загрузи их:"]},{"cell_type":"code","metadata":{"id":"DxeiNcm8aDU4"},"source":["# from google.colab import drive\n","# drive.mount('/content/gdrive/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wXjhtsfF_gBK"},"source":["!gdown https://drive.google.com/uc?id=1eE1FiUkXkcbw0McId4i7qY-L8hH-_Qph&export=download\n","!unzip archive.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sh6wW-K53Mle"},"source":["Заимпортируй библиотеки и сделай работу скриптов вопсроизводимой."]},{"cell_type":"code","metadata":{"id":"A2Y5CHRm6NFe"},"source":["import math\n","import random\n","import string\n","\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","\n","import torch\n","import nltk\n","import gensim\n","import gensim.downloader as api"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"73Lb0wbESrgQ"},"source":["random.seed(42)\n","np.random.seed(42)\n","torch.random.manual_seed(42)\n","torch.cuda.random.manual_seed(42)\n","torch.cuda.random.manual_seed_all(42)\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L_Wv-4bu83Fl"},"source":["data = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding=\"latin\", header=None, names=[\"emotion\", \"id\", \"date\", \"flag\", \"user\", \"text\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ycVFDWeqZ7xI"},"source":["len(data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AAzf236wZ5Cl"},"source":["# data = data[:10000]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RY1pvYDS3Yuj"},"source":["Посмотрим на данные"]},{"cell_type":"code","metadata":{"id":"jST2tjgjCTWD"},"source":["data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kCBwe0wR83C2"},"source":["examples = data[\"text\"].sample(10)\n","print(\"\\n\".join(examples))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GvcYW8aX3mKt"},"source":["Текст очень грязные. Надо добавить очистку текста в его предобработку. \n","\n","Чтобы сравнивать различные методы обработки текста/модели/прочее, разделим датасет на dev(для обучения модели) и test(для получения качества модели)."]},{"cell_type":"code","metadata":{"id":"f8hUK-jnQg6O"},"source":["indexes = np.arange(data.shape[0])\n","np.random.shuffle(indexes)\n","dev_size = math.ceil(data.shape[0] * 0.8)\n","\n","dev_indexes = indexes[:dev_size]\n","test_indexes = indexes[dev_size:]\n","\n","dev_data = data.iloc[dev_indexes]\n","test_data = data.iloc[test_indexes]\n","\n","dev_data.reset_index(drop=True, inplace=True)\n","test_data.reset_index(drop=True, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ivcpeFoCnZA"},"source":["## Обработка текста"]},{"cell_type":"markdown","metadata":{"id":"Df4nca285Dar"},"source":["Стокенизируем текст, избавим от знаков пунктуации и мелких слов."]},{"cell_type":"code","metadata":{"id":"nsNHNDES9ZVF"},"source":["tokenizer = nltk.WordPunctTokenizer()\n","line = tokenizer.tokenize(dev_data[\"text\"][0].lower())\n","print(\" \".join(line))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GcBS_u_hTuxp"},"source":["filtered_line = [w for w in line if all(c not in string.punctuation for c in w) and len(w) > 3]\n","print(\" \".join(filtered_line))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cuFmlXkC6E7X"},"source":["Загрузим предобученную модель эмбеддингов. Если хотите, можно попробовать другую. Полный список можно найти здесь: https://github.com/RaRe-Technologies/gensim-data . "]},{"cell_type":"code","metadata":{"id":"cACJpje2T5bc"},"source":["word2vec = api.load(\"word2vec-google-news-300\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NafmYHrkT5YD"},"source":["emb_line = [word2vec.get_vector(w) for w in filtered_line if w in word2vec]\n","print(sum(emb_line).shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LTS6LCkd6_E7"},"source":["Эмбеддинги не нормализированны, поэтому это тоже надо делать (нейросети и не только любят нормальные данные)."]},{"cell_type":"code","metadata":{"id":"3PyLTZ6xf3Oq"},"source":["mean = np.mean(word2vec.vectors, 0)\n","std = np.std(word2vec.vectors, 0)\n","norm_emb_line = [(word2vec.get_vector(w) - mean) / std for w in filtered_line if w in word2vec and len(w) > 3]\n","print(sum(norm_emb_line).shape)\n","print([all(norm_emb_line[i] == emb_line[i]) for i in range(len(emb_line))])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q7vm6Ppd7Ubw"},"source":["Сделаем датасет, который будет по запросу возвращать подготовленные данные."]},{"cell_type":"code","metadata":{"id":"b4eZajF7pZ1X"},"source":["from torch.utils.data import Dataset, random_split\n","\n","\n","class TwitterDataset(Dataset):\n","    def __init__(self, data: pd.DataFrame, feature_column: str, target_column: str, word2vec: gensim.models.Word2Vec):\n","        self.tokenizer = nltk.WordPunctTokenizer()\n","        \n","        self.data = data\n","\n","        self.feature_column = feature_column\n","        self.target_column = target_column\n","\n","        self.word2vec = word2vec\n","\n","        self.label2num = lambda label: 0 if label == 0 else 1\n","        self.mean = np.mean(self.word2vec.vectors, axis=0)\n","        self.std = np.std(self.word2vec.vectors, axis=0)\n","\n","    def __getitem__(self, item):\n","        # print(self.data[self.feature_column])\n","        text = self.data[self.feature_column][item]\n","        label = self.label2num(self.data[self.target_column][item])\n","\n","        # print('get_tokens_', text)\n","        tokens = self.get_tokens_(text)\n","        # print('get_embeddings_', tokens)\n","        embeddings = self.get_embeddings_(tokens)\n","        # print('return')\n","        return {\"feature\": embeddings, \"target\": label}\n","\n","    def get_tokens_(self, text):\n","        line = tokenizer.tokenize(text.lower())\n","        filtered_line = [w for w in line if all(c not in string.punctuation for c in w) and len(w) > 3]\n","\n","        return filtered_line\n","\n","    def get_embeddings_(self, tokens):\n","\n","        embeddings = [(self.word2vec.get_vector(w) - self.mean) / self.std for w in tokens if w in self.word2vec and len(w) > 3]\n","\n","        if len(embeddings) == 0:\n","            embeddings = np.zeros((1, self.word2vec.vector_size))\n","        else:\n","            embeddings = np.array(embeddings)\n","            if len(embeddings.shape) == 1:\n","                embeddings = embeddings.reshape(-1, 1)\n","\n","        return embeddings\n","\n","    def __len__(self):\n","        return self.data.shape[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IZJpttbXpZyz"},"source":["dev = TwitterDataset(dev_data, \"text\", \"emotion\", word2vec)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4AhHrWa196Yc"},"source":["## Average embedding\n","---\n","Попробуем получить векторное представление предложения из эмбеддингов слов. Самый простой вариант: усреднить вектора по всем словам. Полученный вектор можно отправить любому классификатору как вектор признаков.\n","\n","Посмотрим, насколько хорошо усреднее работает для определение эмоций твитов. Сделаем их визуализацию."]},{"cell_type":"code","metadata":{"id":"1GkavzM9vPKy"},"source":["[print(len(dev))]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ScdokSW-994t"},"source":["indexes = np.arange(len(dev))\n","np.random.shuffle(indexes)\n","example_indexes = indexes[::1000]\n","print(len(example_indexes))\n","\n","examples = {\"features\": [np.sum(dev[i][\"feature\"], axis=0) for i in example_indexes], \n","            \"targets\": [dev[i][\"target\"] for i in example_indexes]}\n","print(len(examples[\"features\"]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LZwFksd_8uYO"},"source":["Для визуализации векторов надо получить их проекцию на плоскость. Сделаем это с помощью `PCA`. Можно получить более аккуратными алгоритмами, но данный алгоритм покажет сложность задачи и поможет оценить требования к классификатору."]},{"cell_type":"code","metadata":{"id":"aKFZRSHdtIac"},"source":["from sklearn.decomposition import PCA\n","\n","\n","pca = PCA(n_components=2)\n","examples[\"transformed_features\"] = pca.fit_transform(examples['features']) # Обучи PCA на эмбеддингах слов"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"szEOWdiNtIX8"},"source":["import bokeh.models as bm, bokeh.plotting as pl\n","from bokeh.io import output_notebook\n","output_notebook()\n","\n","def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n","                 width=600, height=400, show=True, **kwargs):\n","    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n","    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n","\n","    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n","    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n","\n","    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n","    if show: pl.show(fig)\n","    return fig"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7OONK8ldtIWe"},"source":["draw_vectors(\n","    examples[\"transformed_features\"][:, 0], \n","    examples[\"transformed_features\"][:, 1], \n","    color=[[\"red\", \"blue\"][t] for t in examples[\"targets\"]]\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0fNF6LRQ9MPI"},"source":["Скорее всего, ты увидел не сильно различающиеся классы. Возможно, обычный линейный классификатор не очень хорошо справится с задачей. Надо будет делать глубокую(хотя бы два слоя) нейронную сеть.\n","\n","Подготовим загрузчики данных.\n","Усреднее векторов будем делать в \"батчевалке\"(`collate_fn`). Она используется для того, чтобы собирать из данных `torch.Tensor` батчи, которые можно отправлять в модель.\n"]},{"cell_type":"code","metadata":{"id":"y1XapsADtITv"},"source":["from torch.utils.data import DataLoader\n","\n","batch_size = 1024\n","num_workers = 0\n","\n","def average_emb(batch):\n","    # print(batch)\n","    # print(batch[0])\n","    features = [np.mean(b[\"feature\"], axis=0) for b in batch]\n","    targets = [b[\"target\"] for b in batch]\n","\n","    return {\"features\": torch.FloatTensor(features), \"targets\": torch.LongTensor(targets)}\n","\n","\n","train_size = math.ceil(len(dev) * 0.8)\n","\n","train, valid = random_split(dev, [train_size, len(dev) - train_size])\n","\n","train_loader = DataLoader(train, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True, collate_fn=average_emb)\n","valid_loader = DataLoader(valid, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False, collate_fn=average_emb)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GHdmyQKS67Um"},"source":["for batch_idx, data in enumerate(train_loader, 0):\n","    x, y = data\n","    print(data['features'].size())\n","    print(data['targets'])\n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p-zs0WEK-Vkt"},"source":["Для обученния и тестирования нейросетевой модели сделаем отдельные функции."]},{"cell_type":"code","metadata":{"id":"U--T2Gjw1r27"},"source":["from tqdm.notebook import tqdm\n","\n","\n","def training(model, optimizer, criterion, train_loader, epoch, device=\"cpu\"):\n","    print('training')\n","    pbar = tqdm(train_loader, desc=f\"Epoch {e + 1}. Train Loss: {0}\")\n","    model.train()\n","    # print('model.train()')\n","    for batch in pbar:\n","        # print('batch')\n","        # print(batch[\"features\"])\n","\n","        features = batch[\"features\"].to(device)\n","        targets = batch[\"targets\"].to(device)\n","        # print(1111)\n","        # print('preds')\n","        # Получи предсказания модели\n","        preds = model(features)\n","        optimizer.zero_grad()\n","\n","        # print('preds', preds)\n","        # print('targets', targets)\n","        # print(preds.size(), targets.size())\n","        loss = criterion(preds, targets) # Посчитай лосс\n","        # print('loss', loss)\n","        # Обнови параметры модели\n","        loss.backward()\n","        # print('backward')\n","        optimizer.step()\n","        # print('optimizer')\n","        pbar.set_description(f\"Epoch {e + 1}. Train Loss: {loss:.4}\")\n","    \n","\n","def testing(model, criterion, test_loader, device=\"cpu\"):\n","    pbar = tqdm(test_loader, desc=f\"Test Loss: {0}, Test Acc: {0}\")\n","    mean_loss = 0\n","    mean_acc = 0\n","    model.eval()\n","    with torch.no_grad():\n","        for batch in pbar:\n","            features = batch[\"features\"].to(device)\n","            targets = batch[\"targets\"].to(device)\n","\n","            # Получи предсказания модели\n","            preds = model(features) # Посчитай лосс\n","            # print('preds', preds.size())\n","            # print('preds', preds)\n","            # print(np.argmax(preds, axis=1))\n","            # preds = torch.argmax(preds, dim=1)\n","            # # preds = torch.reshape(predsind, (-1,1))\n","            # print('preds', preds)\n","            # print('targets', targets)\n","            loss = criterion(preds, targets) # Посчитай точность модели\n","            # print('loss', loss)\n","\n","            \n","            acc_temp = (preds[:,1] > preds[:,0]).int()\n","            acc = (acc_temp == targets).sum().item()/len(targets)\n","\n","            mean_loss += loss.item() # The item() method extracts the loss’s value as a Python float.\n","            mean_acc += acc\n","\n","            pbar.set_description(f\"Test Loss: {loss:.4}, Test Acc: {acc:.4}\")\n","\n","    pbar.set_description(f\"Test Loss: {mean_loss / len(test_loader):.4}, Test Acc: {mean_acc / len(test_loader):.4}\")\n","\n","    return {\"Test Loss\": mean_loss / len(test_loader), \"Test Acc\": mean_acc / len(test_loader)}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Upjh7KrXIRzY"},"source":["x = torch.FloatTensor([[0.2, 0.1, 0.7],\n","                       [0.6, 0.2, 0.2],\n","                       [0.1, 0.8, 0.1]])\n","\n","y = torch.argmax(x, dim=1)\n","print(torch.reshape(y, (-1,1)))\n","# print(x == y.values)\n","# x[y]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oVg_XBBb-YBH"},"source":["Создадим модель, оптимизатор и целевую функцию. Можете создавать любую модель, с любым количеством слоев, функций активации и прочее.\n"]},{"cell_type":"code","metadata":{"id":"EBoZ4F3Fx1Hm"},"source":["import torch.nn as nn\n","from torch.optim import Adam\n","\n","class TwoLayerNet(nn.Module):\n","  def __init__(self, D_in, H, D_out):\n","    super(TwoLayerNet, self).__init__()\n","    self.linear1 = nn.Linear(D_in,H)\n","    self.linear2 = nn.Linear(H,D_out)\n","    self.softmax = nn.Softmax()\n","  def forward(self,x):\n","    h_relu = self.linear1(x).clamp(min=0)\n","    y_pred = self.linear2(h_relu)\n","    preds = self.softmax(y_pred)\n","    return preds\n","\n","# Не забудь поиграться с параметрами ;)\n","vector_size = dev.word2vec.vector_size\n","# print(vector_size) # 300\n","num_classes = 2\n","lr = 1e-2\n","num_epochs = 2\n","\n","device = \"cuda\"\n","model = TwoLayerNet(vector_size, 20, num_classes).to(device)\n","model = model.cuda()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = Adam(model.parameters(),lr=lr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-AitU8AR-zBj"},"source":["Обучим модель и протестируем её.\n","После каждой эпохи, проверим качество модели по валидационной части датасета. Если метрика стала лучше, будем сохранять модель. Подумай, какая метрика (точность или лосс) будет лучше работать в этой задаче? \n","\n","Здесь и далее реализованно с помощью лосс. Если думаешь, что лучше сравнивать модель через качество, то поменяй код выбора модели."]},{"cell_type":"code","metadata":{"id":"gKhk71Pmx1F1"},"source":["best_metric = np.inf\n","for e in range(num_epochs):\n","    training(model, optimizer, criterion, train_loader, e, device)\n","    log = testing(model, criterion, valid_loader, device)\n","    print(log)\n","    if log[\"Test Loss\"] < best_metric:\n","        torch.save(model.state_dict(), \"model.pt\")\n","        best_metric = log[\"Test Loss\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"di4dGwD4x1Dt"},"source":["test_loader = DataLoader(\n","    TwitterDataset(test_data, \"text\", \"emotion\", word2vec), \n","    batch_size=batch_size, \n","    num_workers=num_workers, \n","    shuffle=False,\n","    drop_last=False, \n","    collate_fn=average_emb)\n","\n","model.load_state_dict(torch.load(\"model.pt\", map_location=device))\n","\n","print(testing(model, criterion, test_loader, device=device))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TGhiep9Z-Ata"},"source":["## TF-iDF\n","---\n","\n","Вместо обычного усреднения эмбеддингов их можно дополнительно перевзвесить. Для этого воспользуемся алгоритмом `TD-iDF`. Он уже реализован в библиотеке `scikit-learn`, остается только его добавить в наш пайплайн."]},{"cell_type":"code","metadata":{"id":"zCPPkqA_-CS9"},"source":["from collections import defaultdict\n","from typing import Dict\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","\n","class TwitterDatasetTfIdf(TwitterDataset):\n","    def __init__(self, data: pd.DataFrame, feature_column: str, target_column: str, word2vec: gensim.models.Word2Vec, weights: Dict[str, float] = None):\n","        super().__init__(data, feature_column, target_column, word2vec)\n","\n","        if weights is None:\n","            self.weights = self.get_tf_idf_()\n","        else:\n","            self.weights = weights\n","\n","    def get_embeddings_(self, tokens):\n","        # print('tokens', tokens)\n","        embeddings = [(self.word2vec.get_vector(token) - self.mean) / self.std  * self.weights.get(token, 1) for token in tokens if token in self.word2vec and len(token) > 3]\n","        # print('embeddings', embeddings)\n","        if len(embeddings) == 0:\n","            embeddings = np.zeros((1, self.word2vec.vector_size))\n","        else:\n","            embeddings = np.array(embeddings)\n","            if len(embeddings.shape) == 1:\n","                embeddings = embeddings.reshape(-1, 1)\n","\n","        return embeddings\n","\n","    def get_tf_idf_(self):\n","        # Надо обучить tfidf на очищенном тексте. Но он принимает только список текстов, а не список списка токенов. Надо превратить второе в первое\n","        data_list = self.data[self.feature_column].values.tolist() # Получаем колону текст из каждой строки всего тексту итого массив массивов\n","        filt_list = []\n","        for sent in data_list:\n","            # print('sent', sent)\n","            sent_list = \"\"\n","            token_list = self.get_tokens_(sent)\n","            for tk in token_list:\n","                # print('tk', tk)\n","                if tk in self.word2vec:\n","                    sent_list += tk + ' '\n","            filt_list.append(sent_list)\n","            # print(filt_list)\n","            # break\n","\n","        tf_idf = TfidfVectorizer()\n","        # print(filt_list[:5])\n","\n","        # Обучи tf-idf\n","        tf_idf.fit_transform(filt_list)\n","        # print(dict(zip(tf_idf.get_feature_names(), tf_idf.idf_)))\n","        return dict(zip(tf_idf.get_feature_names(), tf_idf.idf_))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cyuzZ-sujSCV"},"source":["indexes = np.arange(len(dev))\n","np.random.shuffle(indexes)\n","example_indexes = indexes[::100000]\n","print(example_indexes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w0CtoHf_M7xY"},"source":["dev = TwitterDatasetTfIdf(dev_data, \"text\", \"emotion\", word2vec)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S4WzviI8_Ofq"},"source":["Посмотрим на сложность получившейся задачи используя визуализацию через `PCA`."]},{"cell_type":"code","metadata":{"id":"weAn94Z1jFTu"},"source":["examples = {\"features\": [np.sum(dev[i][\"feature\"], axis=0) for i in example_indexes], \n","            \"targets\": [dev[i][\"target\"] for i in example_indexes]}\n","\n","print(len(examples[\"features\"]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E7wDp159TTG5"},"source":["from sklearn.decomposition import PCA\n","\n","\n","pca = PCA(n_components=2)\n","examples[\"transformed_features\"] = pca.fit_transform(examples['features'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sKESl5wLTTFm"},"source":["draw_vectors(\n","    examples[\"transformed_features\"][:, 0], \n","    examples[\"transformed_features\"][:, 1], \n","    color=[[\"red\", \"blue\"][t] for t in examples[\"targets\"]]\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RiW3d1mg_Zs7"},"source":["Создать нейросетку, обучим её на этих данных."]},{"cell_type":"code","metadata":{"id":"Nl1FSgfnTTC-"},"source":["from torch.utils.data import DataLoader\n","import torch.nn as nn\n","from torch.optim import Adam\n","\n","batch_size = 1024\n","num_workers = 0\n","\n","train_size = math.ceil(len(dev) * 0.8)\n","\n","train, valid = random_split(dev, [train_size, len(dev) - train_size])\n","\n","train_loader = DataLoader(train, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True, collate_fn=average_emb)\n","valid_loader = DataLoader(valid, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False, collate_fn=average_emb)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IpTbK_TETTBY"},"source":["class TwoLayerNet(nn.Module):\n","  def __init__(self, D_in, H, D_out):\n","    super(TwoLayerNet, self).__init__()\n","    self.linear1 = nn.Linear(D_in,H)\n","    self.linear2 = nn.Linear(H,D_out)\n","    self.softmax = nn.Softmax()\n","  def forward(self,x):\n","    h_relu = self.linear1(x).clamp(min=0)\n","    y_pred = self.linear2(h_relu)\n","    preds = self.softmax(y_pred)\n","    return preds\n","\n","vector_size = dev.word2vec.vector_size\n","num_classes = 2\n","lr = 1e-2\n","num_epochs = 1\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = TwoLayerNet(vector_size,40,num_classes).to(device)\n","model = model.cuda()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = Adam(model.parameters(),lr=lr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g0kO5Z_MTS_S"},"source":["num_epochs = 2\n","\n","best_metric = np.inf\n","for e in range(num_epochs):\n","    training(model, optimizer, criterion, train_loader, e, device)\n","    log = testing(model, criterion, valid_loader, device)\n","    # print(log)\n","    print(testing(model, criterion, valid_loader, device))\n","    print(log)\n","    if log[\"Test Loss\"] < best_metric:\n","        torch.save(model.state_dict(), \"model.pt\")\n","        best_metric = log[\"Test Loss\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ekAnIC9XM7th"},"source":["test = TwitterDatasetTfIdf(test_data, \"text\", \"emotion\", word2vec, weights=dev.weights)\n","\n","test_loader = DataLoader(\n","    test, \n","    batch_size=batch_size, \n","    num_workers=num_workers, \n","    shuffle=False,\n","    drop_last=False, \n","    collate_fn=average_emb)\n","\n","model.load_state_dict(torch.load(\"model.pt\", map_location=device))\n","\n","print(testing(model, criterion, test_loader, device=device))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"07pomK_qAcN4"},"source":["Есть ли разница в качестве между способами? Получилось ли улучшить качество модели?\n","\n","----\n","Разница в качестве есть и возможно это зависит от кол-во текста в обучающей выборке т.к. какие то способы работаю лучше когда есть меньше данных, а какие то лучше когда есть больше данных. Как говорилось на лекции.\n","У меня tf-idf не дал прироста в качестве.\n","\n","----"]},{"cell_type":"markdown","metadata":{"id":"mdBiO_BL_j8z"},"source":["Сделай небольшое исследование:\n","- Попробуй сделать несколько нейросеток в качестве классификатора\n","- Попробуй другие предобученные эмбеддинги\n","- Попробуй очистить текст от ников (\"@username\"), url-ов и других символов\n","\n","Для реализации последнего тебе могут помочь регулярные выражения (`import re`). Напише ниже отчет, что ты попробовал и что получилось.\n","\n","---\n","\n","Я попробовал несколько нейроннок с разным кол-во слоев но т.к. они очень много ресурсов на colab'e занимают остановился на стандартной с одним скрытым слоем.\n","Убрать ники с помощью регэкспа не успел...\n","\n","---"]},{"cell_type":"code","metadata":{"id":"b3azHINnM7qi"},"source":[""],"execution_count":null,"outputs":[]}]}